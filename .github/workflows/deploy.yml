name: Deploy Retail Data Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-mock pandas numpy faker
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Run tests
      run: |
        export PYTHONPATH=$PYTHONPATH:$(pwd)
        pytest
  deploy:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GCP_SA_KEY }}
        export_default_credentials: true
    
    - name: Deploy Cloud Functions
      run: |
        cd cloud_functions/data_generator
        gcloud functions deploy generate-data-function \
          --runtime python39 \
          --trigger-http \
          --entry-point cloud_function_entry \
          --region europe-west1 \
          --memory 2048MB \
          --timeout 540s
    
    - name: Deploy Airflow DAGs
      run: |
        COMPOSER_BUCKET=$(gcloud composer environments describe retail-data-pipeline --location europe-west1 --format="get(config.dagGcsPrefix)")
        gsutil -m cp -r airflow/dags/* ${COMPOSER_BUCKET}
